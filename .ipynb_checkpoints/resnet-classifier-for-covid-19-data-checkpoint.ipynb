{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-15T05:16:53.902541Z",
     "iopub.status.busy": "2021-07-15T05:16:53.902160Z",
     "iopub.status.idle": "2021-07-15T05:16:53.906381Z",
     "shell.execute_reply": "2021-07-15T05:16:53.905774Z",
     "shell.execute_reply.started": "2021-07-15T05:16:53.902506Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2 #image processing library\n",
    "import glob #finding files\n",
    "\n",
    "#Pytorch Machine Learning Library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T05:16:57.626653Z",
     "iopub.status.busy": "2021-07-15T05:16:57.626145Z",
     "iopub.status.idle": "2021-07-15T05:16:57.656960Z",
     "shell.execute_reply": "2021-07-15T05:16:57.656077Z",
     "shell.execute_reply.started": "2021-07-15T05:16:57.626607Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../input/siim-covid19-dataset-256px-jpg/256px/train/train/\"\n",
    "TEST_PATH = \"../input/siim-covid19-dataset-256px-jpg/256px/test/test/\"\n",
    "one_hot = ['Atypical Appearance', 'Indeterminate Appearance', 'Negative for Pneumonia', 'Typical Appearance']\n",
    "\n",
    "train = pd.read_csv(\"../input/siim-covid19-dataset-256px-jpg/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": true
   },
   "source": [
    "# Saving and Preprocessing COVID-19 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-07-14T02:37:04.273380Z",
     "iopub.status.busy": "2021-07-14T02:37:04.272909Z",
     "iopub.status.idle": "2021-07-14T03:12:25.472571Z",
     "shell.execute_reply": "2021-07-14T03:12:25.471478Z",
     "shell.execute_reply.started": "2021-07-14T02:37:04.273335Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating Labels from Data Frame\n",
    "train_labels = pd.get_dummies(train.label_id).values\n",
    "\n",
    "#Creating array of images\n",
    "train_imgs = None\n",
    "for img_id, study_id in train[['ImageInstanceUID', 'StudyInstanceUID']].values:\n",
    "    filepath = glob.glob(TRAIN_PATH + study_id + \"*\" + img_id + \".jpg\")[0]\n",
    "    if train_imgs is not None:\n",
    "        train_imgs = np.concatenate((train_imgs, np.array([cv2.imread(filepath)])))\n",
    "    else:\n",
    "        train_imgs = np.array([cv2.imread(filepath)])\n",
    "\n",
    "np.save('training_labels', train_labels)\n",
    "np.save('training_images', train_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data Since Preprocessing Images is Time Consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T05:17:01.368528Z",
     "iopub.status.busy": "2021-07-15T05:17:01.368208Z",
     "iopub.status.idle": "2021-07-15T05:17:01.384497Z",
     "shell.execute_reply": "2021-07-15T05:17:01.382901Z",
     "shell.execute_reply.started": "2021-07-15T05:17:01.368500Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training_labels.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8c657e464cdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_labels.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_images.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training_labels.npy'"
     ]
    }
   ],
   "source": [
    "train_labels = np.load('training_labels.npy')\n",
    "train_imgs = np.load('training_images.npy')\n",
    "print(train_labels.shape)\n",
    "print(train_imgs.shape)\n",
    "\n",
    "train_dataset = TensorDataset(Tensor(train_images), Tensor(train_labels))\n",
    "train_set, val_set = random_split(train_dataset, [5334, 1000])\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, drop_last=False, pin_memory=True, num_workers=4)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-14T19:16:35.589763Z",
     "iopub.status.busy": "2021-07-14T19:16:35.589388Z",
     "iopub.status.idle": "2021-07-14T19:16:35.595264Z",
     "shell.execute_reply": "2021-07-14T19:16:35.594025Z",
     "shell.execute_reply.started": "2021-07-14T19:16:35.589732Z"
    }
   },
   "outputs": [],
   "source": [
    "#Activation Functions\n",
    "act_fn_by_name = {\n",
    "    \"tanh\": nn.Tanh,\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"leakyrelu\": nn.LeakyReLU,\n",
    "    \"gelu\": nn.GELU\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-14T19:16:56.001306Z",
     "iopub.status.busy": "2021-07-14T19:16:56.000629Z",
     "iopub.status.idle": "2021-07-14T19:16:56.011641Z",
     "shell.execute_reply": "2021-07-14T19:16:56.010622Z",
     "shell.execute_reply.started": "2021-07-14T19:16:56.001250Z"
    }
   },
   "outputs": [],
   "source": [
    "#Individual Blocks within the ResNet Architecture\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, c_in, act_fn, c_out=-1, downsample=False):\n",
    "        super().__init__()\n",
    "        if not downsample:\n",
    "            c_out = c_in\n",
    "        \n",
    "        #Neural Network representing F\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(c_in, c_out, kernel_size=3, padding=1, stride=1 if not downsample else 2, bias=False),\n",
    "            nn.BatchNorm2d(c_out),\n",
    "            act_fun(),\n",
    "            nn.Conv2d(c_out, c_out, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(c_out)\n",
    "        )\n",
    "        \n",
    "        #Downsampling network for the input in order to facilitate F(x) + x\n",
    "        self.downsample = nn.Conv2d(c_in, c_out, kernel_size=1, stride=2) if subsample else None\n",
    "        self.act_fn = act_fn()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.net(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        out = z + x\n",
    "        out = self.act_fn(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-14T19:17:01.320308Z",
     "iopub.status.busy": "2021-07-14T19:17:01.319863Z",
     "iopub.status.idle": "2021-07-14T19:17:01.337916Z",
     "shell.execute_reply": "2021-07-14T19:17:01.336176Z",
     "shell.execute_reply.started": "2021-07-14T19:17:01.320264Z"
    }
   },
   "outputs": [],
   "source": [
    "#ResNet Model Implementation\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_blocks=[3, 3, 3, 3], c_hidden=[64, 128, 256, 1024], act_fn_name='relu'):\n",
    "        super().__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.c_hidden = c_hidden\n",
    "        self.act_fn = act_fn_by_name[act_fn_name]\n",
    "        self.act_fn_name = act_fn_name\n",
    "        self._create_netwwork()\n",
    "        self._init_params()\n",
    "    \n",
    "    def _create_network(self):\n",
    "        \n",
    "        self.input_net = nn.Sequential(\n",
    "            nn.Conv2d(3, self.c_hidden[0], kernel_size=7, padding=3, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(self.c_hidden[0]),\n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=2),\n",
    "            self.act_fn()\n",
    "        )\n",
    "        \n",
    "        blocks = []\n",
    "        for block_idx, block_count in enumerate(self.num_blocks):\n",
    "            for bc in range(block_count):\n",
    "                downsample = (bc == 0 and block_idx > 0)\n",
    "                blocks.append(\n",
    "                    ResNetBlock(c_in=self.c_hidden[block_idx if not downsample else (block_idx - 1)],\n",
    "                                act_fn=self.act_fn,\n",
    "                                c_out=c_hidden[block_idx],\n",
    "                                downsample=downsample))\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        \n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(c_hidden[-1], self.num_classes)\n",
    "        )\n",
    "    \n",
    "    def _init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity=self.act_fn_name)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_net(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.output_net(x)\n",
    "        return x\n",
    "            \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding a Device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, nb_epochs, lr=0.01):\n",
    "    model.to(device)\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(nb_epochs):\n",
    "        #Training the model on the given batch\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            #Calculating forward pass of the model\n",
    "            output = model(x)\n",
    "            \n",
    "            #Calculating loss\n",
    "            loss = loss_module(output, y)\n",
    "            train_losses.append(loss)\n",
    "            #Cleaning gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Backwward propagation of partial gradients\n",
    "            loss_module.backwward()\n",
    "            optimizer.step()\n",
    "        #Same steps, but calculating the validation loss\n",
    "        val_losses = []\n",
    "        for batch in val_loader:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            loss = loss_module(model(x), y)\n",
    "            val_losses.append(loss)\n",
    "        \n",
    "        print(\"Epoch: \" + str(epoch + 1) + \" Train Loss: \" + str(torch.tensor(train_losses).mean()) + \" Validation Loss: \" + str(torch.tensor(val_losses).mean()))                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
